{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from utils import check_device\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from Explanation.evaluation.tools import CausalMetric\n",
    "device = check_device()\n",
    "experiment_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "import pickle\n",
    "adversarial_config = toml.load(\"Config/adversarial.toml\")\n",
    "explanation_config = toml.load(\"Config/explanation.toml\")\n",
    "mutants_config = toml.load(\"Config/mutants.toml\")\n",
    "pruning_config = toml.load(\"Config/pruning.toml\")\n",
    "pipeline_config = toml.load(\"Config/pipeline.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_config(config):\n",
    "    for key in config.keys():\n",
    "        if isinstance(config[key], dict):\n",
    "            eval_config(config[key])\n",
    "        else:\n",
    "            try:\n",
    "                config[key] = eval(config[key])\n",
    "            except:\n",
    "                pass\n",
    "    return config\n",
    "\n",
    "adversarial_config = eval_config(adversarial_config)\n",
    "explanation_config = eval_config(explanation_config)\n",
    "mutants_config = eval_config(mutants_config)\n",
    "pruning_config = eval_config(pruning_config)\n",
    "pipeline_config = eval_config(pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Config': {'model': 'custom', 'dataset': 'cifar10'},\n",
       " 'Explanation': {'fast_ig': {'input': 'test_dataloader',\n",
       "   'save': True,\n",
       "   'evaluate': True},\n",
       "  'saliencymap': {'input': 'test_dataloader', 'save': True, 'evaluate': True}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = importlib.import_module(f'Config.Model.{pipeline_config[\"Config\"][\"model\"]}')\n",
    "dataset = importlib.import_module(f'Config.Dataset.{pipeline_config[\"Config\"][\"dataset\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComposedModel = model.ComposedModel\n",
    "NUM_CLASSES = model.NUM_CLASSES\n",
    "INPUT_SIZE = model.INPUT_SIZE\n",
    "TYPE = model.TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposedModel().get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset,test_dataset = dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = dataset.BATCH_SIZE\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipeline_config['Config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Explanation': {'fast_ig': {'input': 'test_dataloader',\n",
       "   'save': True,\n",
       "   'evaluate': True},\n",
       "  'saliencymap': {'input': 'test_dataloader', 'save': True, 'evaluate': True}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutants_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    function_config = mutants_config.get(f,{})\n",
    "    function = function(**function_config)\n",
    "    globals()[cfg[\"output\"]] = DataLoader(input_.dataset,batch_size=BATCH_SIZE,shuffle=False,collate_fn=function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    save = cfg[\"save\"]\n",
    "    function_config = adversarial_config.get(f,{})\n",
    "    function = function(model,**function_config)\n",
    "    if f == \"advgan\":\n",
    "        function.train(train_dataloader)\n",
    "    file_name = 0\n",
    "    all_adversarial_labels = list()\n",
    "    all_labels = list()\n",
    "    correct_index = list()\n",
    "    for x,y in input_:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        correct_idx = model(x).argmax(-1) == y\n",
    "        correct_index.append(correct_idx.cpu().detach().numpy())\n",
    "        adversarial_images = function(x,y)\n",
    "        adverserial_labels = model(adversarial_images).argmax(-1).cpu().detach().numpy()\n",
    "        all_labels.append(y.cpu().detach().numpy())\n",
    "        if save:\n",
    "            for adversarial_image in adversarial_images:\n",
    "                adversarial_image = adversarial_image.cpu().detach().numpy().transpose(1,2,0)\n",
    "                adversarial_image = Image.fromarray((adversarial_image * 255).astype(np.uint8))\n",
    "                if not os.path.exists(f\"Outputs/{experiment_name}/{module}/{f}\"):\n",
    "                    os.makedirs(f\"Outputs/{experiment_name}/{module}/{f}\")\n",
    "                adversarial_image.save(f\"Outputs/{experiment_name}/{module}/{f}/{file_name}.png\")\n",
    "                file_name += 1\n",
    "        all_adversarial_labels.append(adverserial_labels)\n",
    "    all_adversarial_labels = np.concatenate(all_adversarial_labels)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    correct_index = np.concatenate(correct_index)\n",
    "    success_rate = (1 - np.sum(all_adversarial_labels[correct_index] == all_labels[correct_index]) / np.sum(correct_index))\n",
    "    print(f\"{f} Success Rate: {success_rate * 100}%\")\n",
    "    if save:\n",
    "        result = {\n",
    "            \"all_adversarial_labels\":all_adversarial_labels,\n",
    "            \"all_labels\":all_labels,\n",
    "            \"success_rate\":success_rate\n",
    "        }\n",
    "        with open(f\"Outputs/{experiment_name}/{module}/{f}/result.pkl\",\"wb\") as f:\n",
    "            pickle.dump(result,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    save = cfg[\"save\"]\n",
    "    function_config = explanation_config.get(f,{})\n",
    "    for i,(x,y) in enumerate(input_):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        if f == \"fast_ig\" or f == \"guided_ig\":\n",
    "            attribution = list()\n",
    "            for x_,y_ in zip(x,y):\n",
    "                x_ = x_.unsqueeze(0)\n",
    "                y_ = y_.unsqueeze(0)\n",
    "                attribution_ = function(model,x_,y_,**function_config)\n",
    "                attribution.append(attribution_)\n",
    "            attribution = np.concatenate(attribution)\n",
    "        else:\n",
    "            attribution = function(model,x,y,**function_config)\n",
    "        if save:\n",
    "            if not os.path.exists(f\"Outputs/{experiment_name}/{module}/{f}\"):\n",
    "                os.makedirs(f\"Outputs/{experiment_name}/{module}/{f}\")\n",
    "            y = y.cpu().detach().numpy()\n",
    "            x = x.cpu().detach().numpy()\n",
    "            np.savez(f\"Outputs/{experiment_name}/{module}/{f}/{i}.npz\",x=x,y=y,attribution=attribution)\n",
    "    if cfg['evaluate']:\n",
    "        results_files = glob.glob(f\"Outputs/{experiment_name}/{module}/{f}/*.npz\")\n",
    "        scores = {'del': [], 'ins': []}\n",
    "        for file in results_files:\n",
    "            results = np.load(file)\n",
    "            attribution = results['attribution']\n",
    "            x = torch.from_numpy(results['x']).to(device)\n",
    "            deletion = CausalMetric(\n",
    "                model, 'del', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)\n",
    "            insertion = CausalMetric(\n",
    "                model, 'ins', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)\n",
    "            scores['del'].extend(deletion.evaluate(\n",
    "                x, attribution, len(x)).tolist())\n",
    "            scores['ins'].extend(insertion.evaluate(\n",
    "                x, attribution, len(x)).tolist())\n",
    "        scores['ins'] = np.array(scores['ins'])\n",
    "        scores['del'] = np.array(scores['del'])\n",
    "        with open(f'Outputs/{experiment_name}/{module}/{f}/scores.txt', 'w') as f:\n",
    "            f.write(\"Insertion: \" + str(scores['ins'].mean()) + \"\\n\")\n",
    "            f.write(\"Deletion: \" + str(scores['del'].mean()) + \"\\n\")\n",
    "            print(\"Insertion: \" + str(scores['ins'].mean()))\n",
    "            print(\"Deletion: \" + str(scores['del'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    function_config = pruning_config.get(f,{})\n",
    "    saved_path = f\"Outputs/{experiment_name}/{module}/{f}/mask.pkl\"\n",
    "    if not f == \"greg\":\n",
    "        function = function(**function_config)\n",
    "        globals()[f\"{f}_pruned_model_pr_{function_config['pruning_ratio']}\"] = function(model,train_dataloader)\n",
    "    else:\n",
    "        function = function(**function_config)\n",
    "        globals()[f\"{f}_pruned_model_pr_{np.mean(function_config['stage_pr'])}\"] = function(model,train_dataloader,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(module):\n",
    "    match module:\n",
    "        case \"Mutants\":\n",
    "            return mutants_runner\n",
    "        case \"Adversarial\":\n",
    "            return adversarial_runner\n",
    "        case \"Pruning\":\n",
    "            return pruning_runner\n",
    "        case \"Explanation\":\n",
    "            return explanation_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fast_ig from Explanation\n",
      "Insertion: 0.09720203146389395\n",
      "Deletion: 0.9859139351174235\n",
      "Running saliencymap from Explanation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zhang\\miniconda3\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion: 0.09720203146389395\n",
      "Deletion: 0.9859139351174235\n"
     ]
    }
   ],
   "source": [
    "for module,func in pipeline_config.items():\n",
    "    for f,cfg in func.items():\n",
    "        md = importlib.import_module(module)\n",
    "        function = getattr(md,f)\n",
    "        print(f\"Running {f} from {module}\")\n",
    "        runner = distribution(module)\n",
    "        runner(module,f,function,cfg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zhang\\miniconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zhang\\miniconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, inception_v3, googlenet, vgg16, mobilenet_v2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import check_device\n",
    "device = check_device()\n",
    "from Explanation.evaluation.tools import CausalMetric\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "model = inception_v3(pretrained=True).eval().to(device)\n",
    "sm = nn.Softmax(dim=-1)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "norm_layer = transforms.Normalize(mean, std)\n",
    "sm = nn.Softmax(dim=-1)\n",
    "model = nn.Sequential(norm_layer, model, sm).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletion = CausalMetric(model, 'del',substrate_fn=torch.zeros_like,hw=224*224,num_classes=1000)\n",
    "insertion = CausalMetric(model, 'ins',substrate_fn=torch.zeros_like,hw=224*224,num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = torch.load(\"img_batch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution = np.load(f\"inception_v3_our_attributions.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 96/225 [05:36<11:23,  5.30s/it]"
     ]
    }
   ],
   "source": [
    "scores = {'del': deletion.evaluate(\n",
    "        img_batch, attribution, 100), 'ins': insertion.evaluate(img_batch, attribution, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
