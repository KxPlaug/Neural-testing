{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import toml\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from utils import check_device\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from inspect import isfunction\n",
    "import torch\n",
    "from Explanation.evaluation.tools import CausalMetric\n",
    "device = check_device()\n",
    "experiment_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "import pickle\n",
    "adversarial_config = toml.load(\"Config/adversarial.toml\")\n",
    "explanation_config = toml.load(\"Config/explanation.toml\")\n",
    "mutants_config = toml.load(\"Config/mutants.toml\")\n",
    "pruning_config = toml.load(\"Config/pruning.toml\")\n",
    "pipeline_config = toml.load(\"Config/pipeline.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_config(config):\n",
    "    for key in config.keys():\n",
    "        if isinstance(config[key], dict):\n",
    "            eval_config(config[key])\n",
    "        else:\n",
    "            try:\n",
    "                if isfunction(eval(config[key])):\n",
    "                    config[key] = eval(config[key])\n",
    "            except:\n",
    "                pass\n",
    "    return config\n",
    "\n",
    "adversarial_config = eval_config(adversarial_config)\n",
    "explanation_config = eval_config(explanation_config)\n",
    "mutants_config = eval_config(mutants_config)\n",
    "pruning_config = eval_config(pruning_config)\n",
    "pipeline_config = eval_config(pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Config': {'model': 'custom', 'dataset': 'cifar10'},\n",
       " 'Pruning': {'greg': {'input': 'train_dataloader',\n",
       "   'output': 'greg_pruned_model_0_65'}},\n",
       " 'Indicator': {'builtin': {'input': 'test_dataloader',\n",
       "   'model': 'greg_pruned_model_0_65'}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = importlib.import_module(f'Config.Model.{pipeline_config[\"Config\"][\"model\"]}')\n",
    "dataset = importlib.import_module(f'Config.Dataset.{pipeline_config[\"Config\"][\"dataset\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = model.TYPE\n",
    "if TYPE == \"Image Classification\":\n",
    "    NUM_CLASSES = model.NUM_CLASSES\n",
    "    INPUT_SIZE = model.INPUT_SIZE\n",
    "elif TYPE == \"Object Detection\":\n",
    "    INPUT_SIZE = model.INPUT_SIZE\n",
    "elif TYPE == \"Text Classification\":\n",
    "    NUM_CLASSES = model.NUM_CLASSES\n",
    "ComposedModel = model.ComposedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposedModel().get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset,test_dataset = dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = dataset.BATCH_SIZE\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipeline_config['Config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pruning': {'greg': {'input': 'train_dataloader',\n",
       "   'output': 'greg_pruned_model_0_65'}},\n",
       " 'Indicator': {'builtin': {'input': 'test_dataloader',\n",
       "   'model': 'greg_pruned_model_0_65'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutants_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    function_config = mutants_config.get(f,{})\n",
    "    function = function(**function_config)\n",
    "    globals()[cfg[\"output\"]] = DataLoader(input_.dataset,batch_size=BATCH_SIZE,shuffle=False,collate_fn=function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    save = cfg[\"save\"]\n",
    "    function_config = adversarial_config.get(f,{})\n",
    "    function = function(model,**function_config)\n",
    "    if f == \"advgan\":\n",
    "        function.train(train_dataloader)\n",
    "    file_name = 0\n",
    "    all_adversarial_labels = list()\n",
    "    all_labels = list()\n",
    "    correct_index = list()\n",
    "    for x,y in input_:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        correct_idx = model(x).argmax(-1) == y\n",
    "        correct_index.append(correct_idx.cpu().detach().numpy())\n",
    "        adversarial_images = function(x,y)\n",
    "        adverserial_labels = model(adversarial_images).argmax(-1).cpu().detach().numpy()\n",
    "        all_labels.append(y.cpu().detach().numpy())\n",
    "        if save:\n",
    "            for adversarial_image in adversarial_images:\n",
    "                adversarial_image = adversarial_image.cpu().detach().numpy().transpose(1,2,0)\n",
    "                adversarial_image = Image.fromarray((adversarial_image * 255).astype(np.uint8))\n",
    "                if not os.path.exists(f\"Outputs/{experiment_name}/{module}/{f}\"):\n",
    "                    os.makedirs(f\"Outputs/{experiment_name}/{module}/{f}\")\n",
    "                adversarial_image.save(f\"Outputs/{experiment_name}/{module}/{f}/{file_name}.png\")\n",
    "                file_name += 1\n",
    "        all_adversarial_labels.append(adverserial_labels)\n",
    "    all_adversarial_labels = np.concatenate(all_adversarial_labels)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    correct_index = np.concatenate(correct_index)\n",
    "    success_rate = (1 - np.sum(all_adversarial_labels[correct_index] == all_labels[correct_index]) / np.sum(correct_index))\n",
    "    print(f\"{f} Success Rate: {success_rate * 100}%\")\n",
    "    if save:\n",
    "        result = {\n",
    "            \"all_adversarial_labels\":all_adversarial_labels,\n",
    "            \"all_labels\":all_labels,\n",
    "            \"success_rate\":success_rate\n",
    "        }\n",
    "        with open(f\"Outputs/{experiment_name}/{module}/{f}/result.pkl\",\"wb\") as f:\n",
    "            pickle.dump(result,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    save = cfg[\"save\"]\n",
    "    function_config = explanation_config.get(f,{})\n",
    "    for i,(x,y) in enumerate(input_):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        if f == \"fast_ig\" or f == \"guided_ig\":\n",
    "            attribution = list()\n",
    "            for x_,y_ in zip(x,y):\n",
    "                x_ = x_.unsqueeze(0)\n",
    "                y_ = y_.unsqueeze(0)\n",
    "                attribution_ = function(model,x_,y_,**function_config)\n",
    "                attribution.append(attribution_)\n",
    "            attribution = np.concatenate(attribution,axis=0)\n",
    "        else:\n",
    "            attribution = function(model,x,y,**function_config)\n",
    "        if save:\n",
    "            if not os.path.exists(f\"Outputs/{experiment_name}/{module}/{f}\"):\n",
    "                os.makedirs(f\"Outputs/{experiment_name}/{module}/{f}\")\n",
    "            y = y.cpu().detach().numpy()\n",
    "            x = x.cpu().detach().numpy()\n",
    "            np.savez(f\"Outputs/{experiment_name}/{module}/{f}/{i}.npz\",x=x,y=y,attribution=attribution)\n",
    "    if cfg['evaluate']:\n",
    "        results_files = glob.glob(f\"Outputs/{experiment_name}/{module}/{f}/*.npz\")\n",
    "        scores = {'del': [], 'ins': []}\n",
    "        for file in results_files:\n",
    "            results = np.load(file)\n",
    "            attribution = results['attribution']\n",
    "            x = torch.from_numpy(results['x']).to(device)\n",
    "            deletion = CausalMetric(\n",
    "                model, 'del', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)\n",
    "            insertion = CausalMetric(\n",
    "                model, 'ins', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)\n",
    "            scores['del'].extend(deletion.evaluate(\n",
    "                x, attribution, len(x)).tolist())\n",
    "            scores['ins'].extend(insertion.evaluate(\n",
    "                x, attribution, len(x)).tolist())\n",
    "        scores['ins'] = np.array(scores['ins'])\n",
    "        scores['del'] = np.array(scores['del'])\n",
    "        with open(f'Outputs/{experiment_name}/{module}/{f}/scores.txt', 'w') as f:\n",
    "            f.write(\"Insertion: \" + str(scores['ins'].mean()) + \"\\n\")\n",
    "            f.write(\"Deletion: \" + str(scores['del'].mean()) + \"\\n\")\n",
    "            print(\"Insertion: \" + str(scores['ins'].mean()))\n",
    "            print(\"Deletion: \" + str(scores['del'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    function_config = pruning_config.get(f,{})\n",
    "    save_path = f\"Outputs/{experiment_name}/{module}/{f}/mask.pkl\"\n",
    "    if not os.path.exists(f\"Outputs/{experiment_name}/{module}/{f}\"):\n",
    "        os.makedirs(f\"Outputs/{experiment_name}/{module}/{f}\")\n",
    "    if not f == \"greg\":\n",
    "        function = function(**function_config)\n",
    "        globals()[cfg[\"output\"]] = function(model,train_dataloader,save_path)\n",
    "    else:\n",
    "        function = function(**function_config)\n",
    "        globals()[cfg[\"output\"]] = function(model,train_dataloader,test_dataloader,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(module):\n",
    "    return_dict = {\n",
    "        \"Mutants\":mutants_runner,\n",
    "        \"Adversarial\":adversarial_runner,\n",
    "        \"Pruning\":pruning_runner,\n",
    "        \"Explanation\":explanation_runner,\n",
    "    }\n",
    "    return return_dict[module]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running greg from Pruning\n",
      "Register layer index and kernel shape:\n",
      "[ 0]               model.conv1 -- kernel_shape: torch.Size([64, 3, 3, 3])\n",
      "[ 1]      model.layer1.0.conv1 -- kernel_shape: torch.Size([64, 64, 1, 1])\n",
      "[ 2]      model.layer1.0.conv2 -- kernel_shape: torch.Size([64, 64, 3, 3])\n",
      "[ 3]      model.layer1.0.conv3 -- kernel_shape: torch.Size([256, 64, 1, 1])\n",
      "[ 4] model.layer1.0.shortcut.0 -- kernel_shape: torch.Size([256, 64, 1, 1])\n",
      "[ 5]      model.layer1.1.conv1 -- kernel_shape: torch.Size([64, 256, 1, 1])\n",
      "[ 6]      model.layer1.1.conv2 -- kernel_shape: torch.Size([64, 64, 3, 3])\n",
      "[ 7]      model.layer1.1.conv3 -- kernel_shape: torch.Size([256, 64, 1, 1])\n",
      "[ 8]      model.layer1.2.conv1 -- kernel_shape: torch.Size([64, 256, 1, 1])\n",
      "[ 9]      model.layer1.2.conv2 -- kernel_shape: torch.Size([64, 64, 3, 3])\n",
      "[10]      model.layer1.2.conv3 -- kernel_shape: torch.Size([256, 64, 1, 1])\n",
      "[11]      model.layer2.0.conv1 -- kernel_shape: torch.Size([128, 256, 1, 1])\n",
      "[12]      model.layer2.0.conv2 -- kernel_shape: torch.Size([128, 128, 3, 3])\n",
      "[13]      model.layer2.0.conv3 -- kernel_shape: torch.Size([512, 128, 1, 1])\n",
      "[14] model.layer2.0.shortcut.0 -- kernel_shape: torch.Size([512, 256, 1, 1])\n",
      "[15]      model.layer2.1.conv1 -- kernel_shape: torch.Size([128, 512, 1, 1])\n",
      "[16]      model.layer2.1.conv2 -- kernel_shape: torch.Size([128, 128, 3, 3])\n",
      "[17]      model.layer2.1.conv3 -- kernel_shape: torch.Size([512, 128, 1, 1])\n",
      "[18]      model.layer2.2.conv1 -- kernel_shape: torch.Size([128, 512, 1, 1])\n",
      "[19]      model.layer2.2.conv2 -- kernel_shape: torch.Size([128, 128, 3, 3])\n",
      "[20]      model.layer2.2.conv3 -- kernel_shape: torch.Size([512, 128, 1, 1])\n",
      "[21]      model.layer2.3.conv1 -- kernel_shape: torch.Size([128, 512, 1, 1])\n",
      "[22]      model.layer2.3.conv2 -- kernel_shape: torch.Size([128, 128, 3, 3])\n",
      "[23]      model.layer2.3.conv3 -- kernel_shape: torch.Size([512, 128, 1, 1])\n",
      "[24]      model.layer3.0.conv1 -- kernel_shape: torch.Size([256, 512, 1, 1])\n",
      "[25]      model.layer3.0.conv2 -- kernel_shape: torch.Size([256, 256, 3, 3])\n",
      "[26]      model.layer3.0.conv3 -- kernel_shape: torch.Size([1024, 256, 1, 1])\n",
      "[27] model.layer3.0.shortcut.0 -- kernel_shape: torch.Size([1024, 512, 1, 1])\n",
      "[28]      model.layer3.1.conv1 -- kernel_shape: torch.Size([256, 1024, 1, 1])\n",
      "[29]      model.layer3.1.conv2 -- kernel_shape: torch.Size([256, 256, 3, 3])\n",
      "[30]      model.layer3.1.conv3 -- kernel_shape: torch.Size([1024, 256, 1, 1])\n",
      "[31]      model.layer3.2.conv1 -- kernel_shape: torch.Size([256, 1024, 1, 1])\n",
      "[32]      model.layer3.2.conv2 -- kernel_shape: torch.Size([256, 256, 3, 3])\n",
      "[33]      model.layer3.2.conv3 -- kernel_shape: torch.Size([1024, 256, 1, 1])\n",
      "[34]      model.layer3.3.conv1 -- kernel_shape: torch.Size([256, 1024, 1, 1])\n",
      "[35]      model.layer3.3.conv2 -- kernel_shape: torch.Size([256, 256, 3, 3])\n",
      "[36]      model.layer3.3.conv3 -- kernel_shape: torch.Size([1024, 256, 1, 1])\n",
      "[37]      model.layer3.4.conv1 -- kernel_shape: torch.Size([256, 1024, 1, 1])\n",
      "[38]      model.layer3.4.conv2 -- kernel_shape: torch.Size([256, 256, 3, 3])\n",
      "[39]      model.layer3.4.conv3 -- kernel_shape: torch.Size([1024, 256, 1, 1])\n",
      "[40]      model.layer3.5.conv1 -- kernel_shape: torch.Size([256, 1024, 1, 1])\n",
      "[41]      model.layer3.5.conv2 -- kernel_shape: torch.Size([256, 256, 3, 3])\n",
      "[42]      model.layer3.5.conv3 -- kernel_shape: torch.Size([1024, 256, 1, 1])\n",
      "[43]      model.layer4.0.conv1 -- kernel_shape: torch.Size([512, 1024, 1, 1])\n",
      "[44]      model.layer4.0.conv2 -- kernel_shape: torch.Size([512, 512, 3, 3])\n",
      "[45]      model.layer4.0.conv3 -- kernel_shape: torch.Size([2048, 512, 1, 1])\n",
      "[46] model.layer4.0.shortcut.0 -- kernel_shape: torch.Size([2048, 1024, 1, 1])\n",
      "[47]      model.layer4.1.conv1 -- kernel_shape: torch.Size([512, 2048, 1, 1])\n",
      "[48]      model.layer4.1.conv2 -- kernel_shape: torch.Size([512, 512, 3, 3])\n",
      "[49]      model.layer4.1.conv3 -- kernel_shape: torch.Size([2048, 512, 1, 1])\n",
      "[50]      model.layer4.2.conv1 -- kernel_shape: torch.Size([512, 2048, 1, 1])\n",
      "[51]      model.layer4.2.conv2 -- kernel_shape: torch.Size([512, 512, 3, 3])\n",
      "[52]      model.layer4.2.conv3 -- kernel_shape: torch.Size([2048, 512, 1, 1])\n",
      "[53]              model.linear -- kernel_shape: torch.Size([10, 2048])\n",
      "{'model.conv1': 0.0, 'model.layer1.0.conv1': 0.675, 'model.layer1.0.conv2': 0.675, 'model.layer1.0.conv3': 0.675, 'model.layer1.0.shortcut.0': 0.675, 'model.layer1.1.conv1': 0.675, 'model.layer1.1.conv2': 0.675, 'model.layer1.1.conv3': 0.675, 'model.layer1.2.conv1': 0.675, 'model.layer1.2.conv2': 0.675, 'model.layer1.2.conv3': 0.675, 'model.layer2.0.conv1': 0.675, 'model.layer2.0.conv2': 0.675, 'model.layer2.0.conv3': 0.675, 'model.layer2.0.shortcut.0': 0.675, 'model.layer2.1.conv1': 0.675, 'model.layer2.1.conv2': 0.675, 'model.layer2.1.conv3': 0.675, 'model.layer2.2.conv1': 0.675, 'model.layer2.2.conv2': 0.675, 'model.layer2.2.conv3': 0.675, 'model.layer2.3.conv1': 0.675, 'model.layer2.3.conv2': 0.675, 'model.layer2.3.conv3': 0.675, 'model.layer3.0.conv1': 0.675, 'model.layer3.0.conv2': 0.675, 'model.layer3.0.conv3': 0.675, 'model.layer3.0.shortcut.0': 0.675, 'model.layer3.1.conv1': 0.675, 'model.layer3.1.conv2': 0.675, 'model.layer3.1.conv3': 0.675, 'model.layer3.2.conv1': 0.675, 'model.layer3.2.conv2': 0.675, 'model.layer3.2.conv3': 0.675, 'model.layer3.3.conv1': 0.675, 'model.layer3.3.conv2': 0.675, 'model.layer3.3.conv3': 0.675, 'model.layer3.4.conv1': 0.675, 'model.layer3.4.conv2': 0.675, 'model.layer3.4.conv3': 0.675, 'model.layer3.5.conv1': 0.675, 'model.layer3.5.conv2': 0.675, 'model.layer3.5.conv3': 0.675, 'model.layer4.0.conv1': 0.675, 'model.layer4.0.conv2': 0.675, 'model.layer4.0.conv3': 0.675, 'model.layer4.0.shortcut.0': 0.675, 'model.layer4.1.conv1': 0.675, 'model.layer4.1.conv2': 0.675, 'model.layer4.1.conv3': 0.675, 'model.layer4.2.conv1': 0.675, 'model.layer4.2.conv2': 0.675, 'model.layer4.2.conv3': 0.675, 'model.linear': 0.675}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa5c28f117142dea032317023b27aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc1 = 95.4000 Acc5 = 99.7700 Iter = 0 (before update) [prune_state = update_reg]\n",
      "==> [0] Just finished 'update_reg'. Iter = 0\n",
      "Acc1 = 95.0400 Acc5 = 99.8400 Iter = 500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.2400 Acc5 = 99.8400 Iter = 1000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.2400 Acc5 = 99.8400 Iter = 1500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.3800 Acc5 = 99.8300 Iter = 2000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.2200 Acc5 = 99.8600 Iter = 2500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.2800 Acc5 = 99.8500 Iter = 3000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.4000 Acc5 = 99.8500 Iter = 3500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.4700 Acc5 = 99.8500 Iter = 4000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.4100 Acc5 = 99.8700 Iter = 4500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.4700 Acc5 = 99.8600 Iter = 5000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.5500 Acc5 = 99.8300 Iter = 5500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.5600 Acc5 = 99.8800 Iter = 6000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.5800 Acc5 = 99.8800 Iter = 6500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.5500 Acc5 = 99.8400 Iter = 7000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.5100 Acc5 = 99.8300 Iter = 7500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.4200 Acc5 = 99.8100 Iter = 8000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.3000 Acc5 = 99.8300 Iter = 8500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.3100 Acc5 = 99.8000 Iter = 9000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.2300 Acc5 = 99.7600 Iter = 9500 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.2900 Acc5 = 99.8100 Iter = 10000 (before update) [prune_state = update_reg]\n",
      "Acc1 = 95.2500 Acc5 = 99.8100 Iter = 10500 (before update) [prune_state = update_reg]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/autodl-tmp/Neural-testing/test.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     runner \u001b[39m=\u001b[39m distribution(module)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     runner(module,f,function,cfg)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     md \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/root/autodl-tmp/Neural-testing/test.ipynb Cell 17\u001b[0m in \u001b[0;36mpruning_runner\u001b[0;34m(module, f, function, cfg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     function \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunction_config)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B123.125.240.150/root/autodl-tmp/Neural-testing/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mglobals\u001b[39m()[cfg[\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m function(model,train_dataloader,test_dataloader,save_path)\n",
      "File \u001b[0;32m~/autodl-tmp/Neural-testing/Pruning/greg.py:168\u001b[0m, in \u001b[0;36mGReg.__call__\u001b[0;34m(self, model, train_loader_prune, test_loader, save_path)\u001b[0m\n\u001b[1;32m    164\u001b[0m passer\u001b[39m.\u001b[39mpick_pruned \u001b[39m=\u001b[39m pick_pruned\n\u001b[1;32m    165\u001b[0m pruner \u001b[39m=\u001b[39m Pruner(model,passer,reg_granularity_recover\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_granularity_recover,reg_granularity_prune\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_granularity_prune,reg_granularity_pick\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_granularity_pick,\n\u001b[1;32m    166\u001b[0m          reg_upper_limit_pick\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_upper_limit_pick,reg_upper_limit\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg_upper_limit,lr_prune\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_prune,momentum\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum,weight_decay\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_decay,\n\u001b[1;32m    167\u001b[0m          test_interval\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_interval,update_reg_interval\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_reg_interval,stabilize_reg_interval\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstabilize_reg_interval)\n\u001b[0;32m--> 168\u001b[0m pruner\u001b[39m.\u001b[39;49mprune()\n\u001b[1;32m    169\u001b[0m mask \u001b[39m=\u001b[39m pruner\u001b[39m.\u001b[39mmask\n\u001b[1;32m    170\u001b[0m pickle\u001b[39m.\u001b[39mdump(mask,\u001b[39mopen\u001b[39m(save_path,\u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/autodl-tmp/Neural-testing/Pruning/reg_pruner.py:234\u001b[0m, in \u001b[0;36mPruner.prune\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    233\u001b[0m \u001b[39m# after backward but before update, apply reg to the grad\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_reg()\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    237\u001b[0m \u001b[39m# change prune state\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/Neural-testing/Pruning/reg_pruner.py:193\u001b[0m, in \u001b[0;36mPruner._apply_reg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m reg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreg[name] \u001b[39m# [N, C]\u001b[39;00m\n\u001b[1;32m    192\u001b[0m reg \u001b[39m=\u001b[39m reg\u001b[39m.\u001b[39mview_as(m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata) \u001b[39m# [N, C, H, W]\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m l2_grad \u001b[39m=\u001b[39m reg \u001b[39m*\u001b[39;49m m\u001b[39m.\u001b[39;49mweight\n\u001b[1;32m    194\u001b[0m m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l2_grad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_module = None\n",
    "last_f = None\n",
    "for module,func in pipeline_config.items():\n",
    "    for f,cfg in func.items():\n",
    "        if module != \"Indicator\":\n",
    "            md = importlib.import_module(module)\n",
    "            function = getattr(md,f)\n",
    "            print(f\"Running {f} from {module}\")\n",
    "            runner = distribution(module)\n",
    "            runner(module,f,function,cfg)\n",
    "        else:\n",
    "            md = importlib.import_module(f\"{module}.{f}\")\n",
    "            metric_function = md.metric\n",
    "            metrics, class_report = metric_function(globals()[cfg[\"model\"]],globals()[cfg[\"input\"]],num_classes=NUM_CLASSES)\n",
    "            if not os.path.exists(f\"Outputs/{experiment_name}/Indicator/{last_module}_{last_f}\"):\n",
    "                os.makedirs(f\"Outputs/{experiment_name}/Indicator/{last_module}_{last_f}\")\n",
    "            metrics.to_csv(f'Outputs/{experiment_name}/Indicator/{last_module}_{last_f}/metrics.csv', index=False)\n",
    "            class_report.to_csv(f'Outputs/{experiment_name}/Indicator/{last_module}_{last_f}/class_report.csv')\n",
    "        last_f = f\n",
    "    last_module = module\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config.Model.vgg16_imagenet import ComposedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466a5cbec12e427a86f390eb095f6fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComposedModel().get_model().num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
