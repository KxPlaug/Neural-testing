{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from utils import check_device\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from Explanation.evaluation.tools import CausalMetric\n",
    "device = check_device()\n",
    "experiment_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "import pickle\n",
    "adversarial_config = toml.load(\"Config/adversarial.toml\")\n",
    "explanation_config = toml.load(\"Config/explanation.toml\")\n",
    "mutants_config = toml.load(\"Config/mutants.toml\")\n",
    "pruning_config = toml.load(\"Config/pruning.toml\")\n",
    "pipeline_config = toml.load(\"Config/pipeline.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_config(config):\n",
    "    for key in config.keys():\n",
    "        if isinstance(config[key], dict):\n",
    "            eval_config(config[key])\n",
    "        else:\n",
    "            try:\n",
    "                config[key] = eval(config[key])\n",
    "            except:\n",
    "                pass\n",
    "    return config\n",
    "\n",
    "adversarial_config = eval_config(adversarial_config)\n",
    "explanation_config = eval_config(explanation_config)\n",
    "mutants_config = eval_config(mutants_config)\n",
    "pruning_config = eval_config(pruning_config)\n",
    "pipeline_config = eval_config(pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Config': {'model': 'example', 'dataset': 'example'},\n",
       " 'Mutants': {'datamissing': {'input': 'test_dataloader',\n",
       "   'output': 'missed_test_dataloader'}},\n",
       " 'Adversarial': {'fgsm': {'input': 'missed_test_dataloader',\n",
       "   'save': True,\n",
       "   'transfer_models': {}},\n",
       "  'bim': {'input': 'test_dataloader', 'save': True}},\n",
       " 'Explanation': {'fast_ig': {'input': 'test_dataloader',\n",
       "   'save': True,\n",
       "   'evaluate': True},\n",
       "  'agi': {'input': 'test_dataloader', 'save': True, 'evaluate': True}},\n",
       " 'Pruning': {'asl': {'input': 'train_dataloader'}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = importlib.import_module(f'Config.Model.{pipeline_config[\"Config\"][\"model\"]}')\n",
    "dataset = importlib.import_module(f'Config.Dataset.{pipeline_config[\"Config\"][\"dataset\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComposedModel = model.ComposedModel\n",
    "NUM_CLASSES = model.NUM_CLASSES\n",
    "INPUT_SIZE = model.INPUT_SIZE\n",
    "TYPE = model.TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposedModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,test_dataset = dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = dataset.BATCH_SIZE\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipeline_config['Config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mutants': {'datamissing': {'input': 'test_dataloader',\n",
       "   'output': 'missed_test_dataloader'}},\n",
       " 'Adversarial': {'fgsm': {'input': 'missed_test_dataloader',\n",
       "   'save': True,\n",
       "   'transfer_models': {}},\n",
       "  'bim': {'input': 'test_dataloader', 'save': True}},\n",
       " 'Explanation': {'fast_ig': {'input': 'test_dataloader',\n",
       "   'save': True,\n",
       "   'evaluate': True},\n",
       "  'agi': {'input': 'test_dataloader', 'save': True, 'evaluate': True}},\n",
       " 'Pruning': {'asl': {'input': 'train_dataloader'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutants_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    function_config = mutants_config.get(f,{})\n",
    "    function = function(**function_config)\n",
    "    globals()[cfg[\"output\"]] = DataLoader(input_.dataset,batch_size=BATCH_SIZE,shuffle=False,collate_fn=function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    save = cfg[\"save\"]\n",
    "    function_config = adversarial_config.get(f,{})\n",
    "    function = function(model,**function_config)\n",
    "    if f == \"advgan\":\n",
    "        function.train(train_dataloader)\n",
    "    file_name = 0\n",
    "    all_adversarial_labels = list()\n",
    "    all_labels = list()\n",
    "    correct_index = list()\n",
    "    for x,y in input_:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        correct_idx = model(x).argmax(-1) == y\n",
    "        correct_index.append(correct_idx.cpu().detach().numpy())\n",
    "        adversarial_images = function(x,y)\n",
    "        adverserial_labels = model(adversarial_images).argmax(-1).cpu().detach().numpy()\n",
    "        all_labels.append(y.cpu().detach().numpy())\n",
    "        if save:\n",
    "            for adversarial_image in adversarial_images:\n",
    "                adversarial_image = adversarial_image.cpu().detach().numpy().transpose(1,2,0)\n",
    "                adversarial_image = Image.fromarray((adversarial_image * 255).astype(np.uint8))\n",
    "                if not os.path.exists(f\"Outputs/{experiment_name}/{module}/{f}\"):\n",
    "                    os.makedirs(f\"Outputs/{experiment_name}/{module}/{f}\")\n",
    "                adversarial_image.save(f\"Outputs/{experiment_name}/{module}/{f}/{file_name}.png\")\n",
    "                file_name += 1\n",
    "        all_adversarial_labels.append(adverserial_labels)\n",
    "    all_adversarial_labels = np.concatenate(all_adversarial_labels)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    correct_index = np.concatenate(correct_index)\n",
    "    success_rate = (1 - np.sum(all_adversarial_labels[correct_index] == all_labels[correct_index]) / np.sum(correct_index))\n",
    "    print(f\"{f} Success Rate: {success_rate * 100}%\")\n",
    "    if save:\n",
    "        result = {\n",
    "            \"all_adversarial_labels\":all_adversarial_labels,\n",
    "            \"all_labels\":all_labels,\n",
    "            \"success_rate\":success_rate\n",
    "        }\n",
    "        with open(f\"Outputs/{experiment_name}/{module}/{f}/result.pkl\",\"wb\") as f:\n",
    "            pickle.dump(result,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    save = cfg[\"save\"]\n",
    "    function_config = explanation_config.get(f,{})\n",
    "    for i,(x,y) in enumerate(input_):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        if f == \"fast_ig\" or f == \"guided_ig\":\n",
    "            attribution = list()\n",
    "            for x_,y_ in zip(x,y):\n",
    "                x_ = x_.unsqueeze(0)\n",
    "                y_ = y_.unsqueeze(0)\n",
    "                attribution_ = function(model,x_,y_,**function_config)\n",
    "                attribution.append(attribution_)\n",
    "            attribution = np.concatenate(attribution)\n",
    "        else:\n",
    "            attribution = function(model,x,y,**function_config)\n",
    "        if save:\n",
    "            if not os.path.exists(f\"Outputs/{experiment_name}/{module}/{f}\"):\n",
    "                os.makedirs(f\"Outputs/{experiment_name}/{module}/{f}\")\n",
    "            y = y.cpu().detach().numpy()\n",
    "            x = x.cpu().detach().numpy()\n",
    "            np.savez(f\"Outputs/{experiment_name}/{module}/{f}/{i}.npz\",x=x,y=y,attribution=attribution)\n",
    "    if cfg['evaluate']:\n",
    "        results_files = glob.glob(f\"Outputs/{experiment_name}/{module}/{f}/*.npz\")\n",
    "        scores = {'del': [], 'ins': []}\n",
    "        for file in results_files:\n",
    "            results = np.load(file)\n",
    "            attribution = results['attribution']\n",
    "            x = torch.from_numpy(results['x']).to(device)\n",
    "            deletion = CausalMetric(\n",
    "                model, 'del', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)\n",
    "            insertion = CausalMetric(\n",
    "                model, 'ins', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)\n",
    "            scores['del'].extend(deletion.evaluate(\n",
    "                x, attribution, len(x)).tolist())\n",
    "            scores['ins'].extend(insertion.evaluate(\n",
    "                x, attribution, len(x)).tolist())\n",
    "        scores['ins'] = np.array(scores['ins'])\n",
    "        scores['del'] = np.array(scores['del'])\n",
    "        with open(f'Outputs/{experiment_name}/{module}/{f}/scores.txt', 'w') as f:\n",
    "            f.write(\"Insertion: \" + str(scores['ins'].mean()) + \"\\n\")\n",
    "            f.write(\"Deletion: \" + str(scores['del'].mean()) + \"\\n\")\n",
    "            print(\"Insertion: \" + str(scores['ins'].mean()))\n",
    "            print(\"Deletion: \" + str(scores['del'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.load(glob.glob(f\"Outputs/20230613-201820/Explanation/agi/*.npz\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution = results['attribution']\n",
    "x = torch.from_numpy(results['x']).to(device)\n",
    "deletion = CausalMetric(\n",
    "    model, 'del', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)\n",
    "insertion = CausalMetric(\n",
    "    model, 'ins', substrate_fn=torch.zeros_like, hw=np.prod(INPUT_SIZE[-2:]), num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4785016847890802"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deletion.evaluate(x, attribution, len(x)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_runner(module,f,function,cfg):\n",
    "    input_ = globals()[cfg[\"input\"]]\n",
    "    function_config = pruning_config.get(f,{})\n",
    "    saved_path = f\"Outputs/{experiment_name}/{module}/{f}/mask.pkl\"\n",
    "    if not f == \"greg\":\n",
    "        function = function(**function_config)\n",
    "        globals()[f\"{f}_pruned_model_pr_{function_config['pruning_ratio']}\"] = function(model,train_dataloader)\n",
    "    else:\n",
    "        function = function(**function_config)\n",
    "        globals()[f\"{f}_pruned_model_pr_{np.mean(function_config['stage_pr'])}\"] = function(model,train_dataloader,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(module):\n",
    "    match module:\n",
    "        case \"Mutants\":\n",
    "            return mutants_runner\n",
    "        case \"Adversarial\":\n",
    "            return adversarial_runner\n",
    "        case \"Pruning\":\n",
    "            return pruning_runner\n",
    "        case \"Explanation\":\n",
    "            return explanation_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running datamissing from Mutants\n",
      "Running fgsm from Adversarial\n",
      "fgsm Success Rate: 100.0%\n",
      "Running bim from Adversarial\n",
      "bim Success Rate: 96.875%\n",
      "Running fast_ig from Explanation\n",
      "Insertion: 0.0007565659075225994\n",
      "Deletion: 0.4785016847890802\n",
      "Running agi from Explanation\n",
      "Insertion: 0.0007565659075225994\n",
      "Deletion: 0.4785016847890802\n",
      "Running asl from Pruning\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ASL.__call__() missing 1 required positional argument: 'save_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m runner \u001b[39m=\u001b[39m distribution(module)\n\u001b[1;32m----> 7\u001b[0m runner(module,f,function,cfg)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mpruning_runner\u001b[1;34m(module, f, function, cfg)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgreg\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m      6\u001b[0m     function \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunction_config)\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mglobals\u001b[39m()[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m_pruned_model_pr_\u001b[39m\u001b[39m{\u001b[39;00mfunction_config[\u001b[39m'\u001b[39m\u001b[39mpruning_ratio\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m function(model,train_dataloader)\n\u001b[0;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     function \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunction_config)\n",
      "\u001b[1;31mTypeError\u001b[0m: ASL.__call__() missing 1 required positional argument: 'save_path'"
     ]
    }
   ],
   "source": [
    "for module,func in pipeline_config.items():\n",
    "    for f,cfg in func.items():\n",
    "        md = importlib.import_module(module)\n",
    "        function = getattr(md,f)\n",
    "        print(f\"Running {f} from {module}\")\n",
    "        runner = distribution(module)\n",
    "        runner(module,f,function,cfg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
